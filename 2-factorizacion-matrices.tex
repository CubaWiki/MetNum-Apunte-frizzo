% -*- root: apunte-metodos.tex -*-

\section{Factorización de matrices}
\label{section:factorizacion-matrices}

En esta sección se presentarán tres maneras distintas de factorizar matrices.
La primera de ellas, la \textbf{factorización de Cholesky}, solo puede
aplicarse a matrices que cumplen con determinadas características; las otras
dos, la \textbf{factorización QR} y la \textbf{descomposición en valores
singulares} o \textbf{SVD}, son aplicables a matrices arbitrarias.

Una de las principales razones por las que estas factorizaciones resultan
útiles es porque proporcionan formas de resolver sistemas de ecuaciones
lineales que tienen propiedades deseables, como eficiencia, reutilizabilidad
y estabilidad numérica. No obstante, las factorizaciones QR y SVD también
tienen aplicaciones a la hora de resolver otros tipos de problemas numéricos,
como el problema de cuadrados mínimos lineales, que es abordado en la
sección \ref{section:cml}.

\subsection{Factorización de Cholesky}

Decimos que una matriz $\mat{A} \in \reals^{n \times n}$ es
\textbf{simétrica} si $\mat{A} = \mat{A}\trans$.
Si una matriz simétrica $\mat{A}$ es inversible y admite factorización LU,
entonces también admite un tipo de factorización que llamaremos LDL.
La misma consiste en una escritura
\[ \mat{A} = \mat{L} \cdot \mat{D} \cdot \mat{L}\trans \]
donde $\mat{L}$ es triangular inferior con unos en la diagonal y $\mat{D}$
es una matriz diagonal.

La demostración proviene de considerar la factorización LU de $\mat{A}$,
$\mat{A} = \mat{L} \cdot \mat{U}$. Como $\mat{A}$ es simétrica, entonces
$\mat{L} \cdot \mat{U} = (\mat{L} \cdot \mat{U})\trans =
\mat{U}\trans \cdot \mat{L}\trans$. Como $\mat{L}$ tiene unos en la diagonal
debe ser inversible y, como $\mat{A}$ es inversible, $\mat{U}$ tiene que
serlo también, de donde % TODO: Terminar esta demostración.


Por otro lado, decimos que una matriz $\mat{A} \in \reals^{n \times n}$
es (simétrica y) \textbf{definida positiva} (s.d.p.) si es
simétrica\footnote{Algunos autores definen la noción de matriz definida
positiva independientemente de la matriz simétrica, mientras que otros
requieren la simetría como una condición para que una matriz sea definida
positiva; aquí se tomó el primer criterio, que es el adoptado por
\emph{Numerical Analysis} (9th Edition) de Richard L. Burden y J. Douglas
Faires.}
y para todo $x \in \reals^{n}$ no nulo se cumple que
$x\trans \cdot \mat{A} \cdot x > 0$. Las matrices definidas positivas siempre son inversibles, y tienen elementos positivos en su diagonal.

Alternativamente, las matrices definidas positivas pueden caracterizarse como
aquellas matrices simétricas cuyos \textbf{menores
principales} son todos positivos. Los menores principales de una matriz
son los determinantes de sus submatrices principales, es decir,
aquellas submatrices cuadradas que contienen su esquina superior izquierda.
A partir de esta caracterización, se puede demostrar que las matrices
definidas positivas siempre admiten factorización LU, y por lo tanto,
al ser simétricas, poseen una factorización LDL.

Dada una matriz $\mat{A}$, llamamos \textbf{factorización de Cholesky} a su
escritura en la forma
\[ \mat{A} = \mat{L} \cdot \mat{L}\trans \]
donde $\mat{L}$ es una matriz triangular inferior con elementos positivos
en la diagonal. A continuación demostraremos que una matriz es
definida positiva si y solo si tiene factorización de Cholesky.

\begin{itemize}
\item[$(\Rightarrow)$] Consideremos una matriz definida positiva $\mat{A} \in
    \reals^{n \times n}$. Como enunciamos anteriormente, dicha matriz debe
    admitir una factorización LDL,
    $\mat{A} = \mat{L} \cdot \mat{D} \cdot \mat{L}\trans$.

    En primer lugar, se puede verificar que los elementos de la diagonal de
    $\mat{D}$ deben ser positivos.
    Basta considerar $i \in \{1, \dots, n\}$ cualquiera; como
    $\mat{L}\trans$ es inversible, se
    puede tomar $x \in \reals^n$ tal que $\mat{L}\trans \cdot x = e_i$, donde
    $e_i$ es un vector que tiene un uno en la posición $i$-ésima y ceros en
    las posiciones restantes. Luego,
    \[ \begin{aligned}
        0 &< x\trans \cdot \mat{A} \cdot x \\
          &= x\trans \cdot \mat{L} \cdot \mat{D} \cdot \mat{L}\trans \cdot x \\
          &= (\mat{L}\trans \cdot x)\trans \cdot \mat{D} \cdot
             (\mat{L}\trans \cdot x) \\
          &= e_i\trans \cdot \mat{D} \cdot e_i \\
          &= d_{i,i}.
    \end{aligned} \]

    Esto permite definir
    \[ \mat{\sqrt{D}} = \begin{bmatrix}
        \sqrt{d_{1,1}} & 0              & \cdots & 0 \\
        0              & \sqrt{d_{2,2}} &        & 0 \\
        \vdots         &                & \ddots & \vdots \\
        0              & 0              & \cdots & \sqrt{d_{n,n}}
    \end{bmatrix} \]
    de modo que $\mat{D} = \mat{\sqrt{D}} \cdot \mat{\sqrt{D}} =
    \mat{\sqrt{D}} \cdot \mat{\sqrt{D}}\trans$. Así, reemplazando en la
    escritura LDL, obtenemos
    \[ \begin{aligned} \mat{A}
        &= \mat{L} \cdot \mat{D} \cdot \mat{L}\trans \\
        &= \mat{L} \cdot \mat{\sqrt{D}} \cdot \mat{\sqrt{D}}\trans
            \cdot \mat{L}\trans \\
        &= (\mat{L} \cdot \mat{\sqrt{D}}) \cdot
            (\mat{L} \cdot \mat{\sqrt{D}})\trans. \\
    \end{aligned} \]

    En esta última escritura, por las características de $\mat{L}$ y
    $\mat{\sqrt{D}}$, la matriz $(\mat{L} \cdot \mat{\sqrt{D}})$ resulta
    triangular inferior con elementos positivos en la diagonal; por lo tanto,
    se trata de una factorización de Cholesky para $\mat{A}$.

\item[$(\Leftarrow)$] Si $\mat{A} = \mat{L} \cdot \mat{L}\trans$, entonces
    necesariamente $\mat{A}$ es simétrica. Basta ver que para todo $x \neq 0$,
    $x\trans \cdot \mat{A} \cdot x > 0$.
    \[ x\trans \cdot \mat{A} \cdot x
        \ = \ x\trans \cdot \mat{L} \cdot \mat{L}\trans \cdot x
        \ = \ \left( \mat{L}\trans \cdot x \right)\trans \cdot
            \left( \mat{L}\trans \cdot x \right)
        \ = \ \Vert \mat{L}\trans \cdot x \Vert_2^2, \]
    y al ser $\mat{L}\trans$ inversible, $\mat{L}\trans \cdot x \neq 0$,
    por lo que la norma necesariamente es positiva.  
\end{itemize}

Una forma de computar la factorización de Cholesky de una matriz $\mat{A}$
surge de observar cómo se podría recuperar un elemento de $\mat{A}$ si ya
se contara con dicha factorización. Sea $\mat{A} = \mat{L} \cdot
\mat{L}\trans$ y consideremos, sin pérdida de generalidad, $i \geq j$; se puede notar que entonces
\[ a_{i,j} = \fil_i(\mat{L}) \cdot \col_j(\mat{L}\trans)
           = \sum_{k=1}^j l_{i,k} \cdot l_{j,k}. \]

La idea es recorrer la matriz $\mat{L}$ de forma ordenada, por columnas,
e ir utilizando la ecuación anterior para obtener, a partir de $\mat{A}$,
los valores que corresponden en cada posición.
\begin{itemize}
\item Para computar la esquina superior izquierda de $\mat{L}$, tenemos en
    cuenta que $a_{1,1} = (l_{1,1})^2$. Por lo tanto,
        \[ l_{1,1} = \sqrt{a_{1,1}}. \]
\item Para completar la primer columna, observamos que si $i > 1$, entonces
    $a_{i,1} = l_{i,1} \cdot l_{1,1}$. Luego,
    \[ l_{i,1} = \frac{a_{i,1}}{l_{1,1}}. \]
\item Consideremos ahora una columna $j > 1$. Como $\mat{L}$ es triangular
    inferior, el primer elemento a computar es el de la diagonal. Podemos
    observar que $a_{j,j} = \sum_{k=1}^j (l_{j,k})^2 =
    \sum_{k=1}^{j-1} (l_{j,k})^2 + (l_{j,j})^2$. Entonces,
    \[ l_{j,j} = \sqrt{a_{j,j} - \sum_{k=1}^{j-1} (l_{j,k})^2}. \]
\item Para calcular los restantes elementos, si $1 < j < i$, tenemos
    $a_{i,j} = \sum_{k=1}^j l_{i,k} \cdot l_{j,k} =
    \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} + l_{i,j} \cdot l_{j,j}$. Así,
    obtenemos que
    \[ l_{i,j} = \frac{1}{l_{j,j}} \cdot
        \left( a_{i,j} - \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} \right). \]
\end{itemize}

Si los pasos se siguen en orden, en cada uno de ellos solo se utilizan
elementos de $\mat{L}$ que ya fueron calculados previamente. Por lo tanto,
tenemos un algoritmo bien definido para obtener la factorización de Cholesky
de cualquier matriz definida positiva.
Además, el algoritmo determina $\mat{L}$ de forma
unívoca partiendo de una ecuación que debe ser satisfecha por toda
factorización de Cholesky de $\mat{A}$; esto demuestra que la factorización
de Cholesky es única.

A continuación, se presenta el algoritmo escrito en forma de pseudocódigo.

\begin{algorithm}[H]
\caption{Factorización de Cholesky}
\label{algo:cholesky}

\Input{$\mat{A} \in \reals^{n \times n}$ definida positiva.}
\Output{$\mat{L}$ triangular superior, con elementos positivos en la
    diagonal, tal que $\mat{A} = \mat{L} \cdot \mat{L}\trans$.}

$\displaystyle l_{1,1} \gets \sqrt{a_{1,1}}$\;
\For {$i=2,\dots,n$} {
    $\displaystyle l_{i,1} \gets \frac{a_{i,1}}{l_{1,1}}$\;
}
\For {$j=2,\dots,n$} {
    $\displaystyle l_{j,j} \gets \sqrt{a_{j,j} -
        \sum_{k=1}^{j-1} (l_{j,k})^2}$\;
    \For {$i=j+1,\dots,n$} {
        $\displaystyle l_{i,j} \gets \frac{1}{l_{j,j}} \cdot
        \left( a_{i,j} - \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} \right)$\;
    }
}
\end{algorithm}

Puede observarse que la complejidad del algoritmo es $\ord{n^3}$. Si bien
se trata de la misma complejidad asintótica que la de obtener una
factorización LU, las constantes son mejores, debido a que la cantidad de
operaciones de punto flotante que deben efectuarse es aproximadamente la
mitad.

% TODO: ¿Cuáles son las bondades de Cholesky?

\subsection{Factorización QR}

Dos vectores $x, y \in \reals^{n}$ se dicen \textbf{ortogonales} ($x \bot y$)
si su producto interno $\langle x,y \rangle$ es $0$. Un conjunto de vectores
es \textbf{ortogonal} si sus elementos son ortogonales dos a dos. Un conjunto
de vectores es \textbf{ortonormal} si es ortogonal y la norma de todos sus
elementos es $1$. Los elementos de un conjunto ortonormal siempre son
linealmente independientes.

Decimos que una matriz $\mat{Q} \in \reals^{n \times n}$ es \textbf{ortogonal}
si sus columnas forman un conjunto ortonormal. Se puede demostrar, además, que
una matriz es ortogonal si y solo si es inversible y su transpuesta es igual a
su inversa, es decir, $\mat{Q}\trans = \mat{Q}^{-1}$. El producto de matrices
ortogonales es también ortogonal, es decir, si $\mat{Q_1}$ y $\mat{Q_2}$ son
ortogonales, $\mat{Q_1} \cdot \mat{Q_2}$ es ortogonal.

Las matrices ortogonales poseen la importante propiedad de estar asociadas a
las
transformaciones lineales que \emph{preservan norma 2}. En otras palabras, una
matriz $\mat{Q} \in \reals^{n \times n}$ es ortogonal si y solo si, para todo
$x \in \reals^n$, $\lVert \mat{Q} \cdot x \rVert_2 = \lVert x \rVert_2$.
De esto se desprende también que el número de condición de una matriz
ortogonal (con respecto a la norma 2) es $\kappa(\mat{Q}) = 1$. Todo esto nos
indica que las matrices ortogonales son muy estables numéricamente, lo cual
las vuelve muy atractivas a la hora de resolver problemas en forma
computacional.
% TODO: MUY IMPORTANTE. En algún lado hay que hablar de normas y de número
% de condición. Por ahí el mejor lugar es acá mismo.


Dada una matriz $\mat{A} \in \reals^{n \times n}$, una \textbf{factorización QR} de $\mat{A}$ es una escritura
\[ \mat{A} = \mat{Q} \cdot \mat{R} \]
donde $\mat{Q} \in \reals^{n \times n}$ es ortogonal y
$\mat{R} \in \reals^{n \times n}$ es triangular superior. Toda matriz admite
una factorización QR, como se verá más adelante al analizar los algoritmos
que permiten obtenerla.

Una de las aplicaciones clave de esta factorización es la resolución
de sistemas de ecuaciones lineales. Si se tiene el sistema $\mat{A} \cdot x =
b$, utilizando la factorización QR, se lo puede reescribir como
\[\mat{Q} \cdot \mat{R} \cdot x = b\]
Al ser $\mat{Q}$ ortogonal, se puede multiplicar
a ambos lados por $\mat{Q}\trans$ y se tiene
\[\mat{Q}\trans \mat{Q} \cdot \mat{R} \cdot x = \mat{Q}\trans b\]
o lo que es lo mismo,
\[\mat{R} \cdot x = \mat{Q}\trans \cdot b\]
Como $\mat{R}$ es triangular superior, este sistema se puede resolver de forma
eficiente con sustitución hacia atrás, con la importante ventaja de que
$\mat{Q}\trans$ es sumamente estable, evitando la introducción de errores
numéricos al calcular el producto con $b$.

\subsubsection{Método de Householder (Reflexiones)}
\subsubsection{Método de Givens (Rotaciones)}
\subsection{Descomposición en valores singulares}

% TODO: Autovalores y autovectores va a haber que explicarlo acá.

Sea $\mat{A} \in \reals^{m \times n}$ una matriz cualquiera, con $\rg(A) = r$.
Entonces, existen
\begin{itemize}
\item una base ortonormal $\lbrace v_1, v_2, \dots, v_n \rbrace$ de $\reals^n$, y
\item una base ortonormal $\lbrace u_1, u_2, \dots, u_m \rbrace$ de $\reals^m$
\end{itemize}
tales que
\[ \mat{U}\trans \cdot \mat{A} \cdot \mat{V} = \mat{\Sigma} = \begin{bmatrix}
\sigma_1 & 0        & \cdots   &    &          &          &          \\
0        & \sigma_2 &          &          &          &          &          \\
\vdots   &          & \ddots   &          &          &          &          \\
         &          &          & \sigma_r &          &          &          \\
         &          &          &          & 0        &          &          \\
         &          &          &          &          & \ddots   &          \\
         &          &          &          &          &          & 0
\end{bmatrix} \]
donde
\begin{itemize}
\item $\mat{V} = \begin{bmatrix}
            &     &        &     \\
        v_1 & v_2 & \dots  & v_n \\
            &     &        &     \\
    \end{bmatrix} \in \reals^{n \times n}$ y
    $\mat{U} = \begin{bmatrix}
            &     &        &     \\
        u_1 & u_2 & \dots  & u_m \\
            &     &        &     \\
    \end{bmatrix} \in \reals^{m \times m}$ son ortogonales, y
\item $\mat{\Sigma} \in \reals^{m \times n}$ es una matriz diagonal,
    con $\sigma_i > 0 \fall{i = 1, \dots, r}$.
\end{itemize}

Los valores $\sigma_i$ se denominan los \textbf{valores singulares} de \mat{A}.

\subsubsection{Demostración de la existencia}

Queremos hallar $\mat{U}$, $\mat{V}$, $\mat{\Sigma}$ que cumplan la igualdad
enunciada:
\[ \mat{U}\trans \cdot \mat{A} \cdot \mat{V} = \mat{\Sigma} \]
o, equivalentemente (usando la ortogonalidad de $\mat{U}$ y $\mat{V}$),
\begin{multicols}{2}\noindent
\[ \begin{aligned} \mat{A} \cdot \mat{V} &= \mat{U} \cdot \mat{\Sigma} \\
    & \Updownarrow \\
    \mat{A} \cdot \begin{bmatrix}
            &     &        &     \\
        v_1 & v_2 & \dots  & v_n \\
            &     &        &     \\
    \end{bmatrix} &= \begin{bmatrix}
            &     &        &     \\
        u_1 & u_2 & \dots  & u_m \\
            &     &        &     \\
    \end{bmatrix} \cdot \mat{\Sigma} \\
    & \Updownarrow
\end{aligned}
\]
\begin{numcases}{}
    \mat{A} \cdot v_i = \sigma_i \cdot u_i & para $i = 1,\dots,r$
        \label{eq:svd:caso1} \\
    \mat{A} \cdot v_i = 0                  & para $i = r+1,\dots,n$
        \label{eq:svd:caso2}
\end{numcases}

\columnbreak \noindent
\[\begin{aligned} \mat{A}\trans \cdot \mat{U} &= \mat{V} \cdot \mat{\Sigma} \\
    & \Updownarrow \\
    \mat{A}\trans \cdot \begin{bmatrix}
            &     &        &     \\
        u_1 & u_2 & \dots  & u_m \\
            &     &        &     \\
    \end{bmatrix} &= \begin{bmatrix}
            &     &        &     \\
        v_1 & v_2 & \dots  & v_n \\
            &     &        &     \\
    \end{bmatrix} \cdot \mat{\Sigma} \\
    & \Updownarrow
\end{aligned}
\]
\begin{numcases}{}
    \mat{A}\trans \cdot u_i = \sigma_i \cdot v_i & para $i = 1,\dots,r$
        \label{eq:svd:caso3} \\
    \mat{A}\trans \cdot u_i = 0                  & para $i = r+1,\dots,m$
        \label{eq:svd:caso4}
\end{numcases}

\end{multicols}

Buscaremos definir
\begin{itemize}
\item $v_1, \dots, v_n \in \reals^n$ ortonormales,
\item $u_1, \dots, u_m \in \reals^m$ ortonormales, y
\item $\sigma_1, \dots, \sigma_r > 0$
\end{itemize}
que cumplan las ecuaciones (\ref{eq:svd:caso1}), (\ref{eq:svd:caso2}),
(\ref{eq:svd:caso3}) y (\ref{eq:svd:caso4}).

En primer lugar, vemos que:
\begin{enumerate}[label=(\roman*)]
\item Para $i = 1, \dots, r$,
\[ \begin{aligned}
    \mat{A}\trans \cdot \mat{A} \cdot v_i &= \sigma_i \cdot \mat{A}\trans \cdot u_i \\
    \mat{A}\trans \cdot \mat{A} \cdot v_i &= \sigma_i^2 \cdot v_i \\
    \end{aligned} \]
es decir, los $v_i$ deberán ser autovectores de $\mat{A}\trans \cdot \mat{A}$,
con autovalor $\sigma_i^2$.
\item Para $i = r+1, \dots, n$,
\[ \mat{A}\trans \cdot \mat{A} \cdot v_i = \mat{A}\trans \cdot 0 = 0 \]
es decir, los $v_i$ deberán ser autovectores de $\mat{A}\trans \cdot \mat{A}$,
con autovalor $0$.
\end{enumerate}

Como $\mat{A}\trans \cdot \mat{A}$ es simétrica, existe una base ortonormal de
$\reals^n$ compuesta por autovectores de $\mat{A}\trans \cdot \mat{A}$.
Además, por ser semidefinida positiva, todos sus autovalores son no negativos.

Por otra parte, tenemos que
\[ \fall{x \in \reals^n} \mat{A} \cdot x = 0 \Leftrightarrow
    \mat{A}\trans \cdot \mat{A} \cdot x \]
dado que, trivialmente, $\mat{A} \cdot x = 0 \Rightarrow
\mat{A}\trans \cdot \mat{A} \cdot x = 0$, mientras que
$ \mat{A}\trans \cdot \mat{A} \cdot x = 0
    \Rightarrow x\trans \cdot \mat{A}\trans \cdot \mat{A} \cdot x = 0
    \Rightarrow \lVert \mat{A} \cdot x \rVert = 0
    \Rightarrow \mat{A} \cdot x = 0$.
Así, $\rg(\mat{A}\trans \cdot \mat{A}) = \rg(\mat{A}) = 0$, lo cual significa
que $\mat{A}\trans \cdot \mat{A}$ tiene exactamente $n - r$ autovectores
asociados al autovalor $0$.

Definimos entonces $\lbrace v_1, \dots, v_n \rbrace$ como una base ortonormal
de autovectores de $\mat{A}\trans \cdot \mat{A}$, ordenados de modo que los
últimos $n-r$ son los asociados al autovalor $0$. Veamos que estos $v_i$
cumplen con las condiciones necesarias.

\begin{enumerate}[label=(\roman*)]
\item Para $i = 1, \dots, r$, sea $\lambda_i$ el autovalor de $\mat{A}\trans
    \cdot \mat{A}$ asociado al autovector $v_i$. Sabemos que $\lambda_i > 0$.
    Definimos entonces:
    \begin{itemize}
    \item $\sigma_i = \sqrt{\lambda_i} > 0$.
    \item $\displaystyle u_i = \frac{\mat{A} \cdot v_i}{\sigma_i}$.
        Trivialmente, puede verse que satisfacen la ecuación
        (\ref{eq:svd:caso1}).
    \end{itemize}

    Veamos que $u_1, \dots, u_r$ son ortonormales:
    \begin{itemize}
    \item $\displaystyle \lVert u_i \rVert_2^2
        = \left( \frac{\mat{A} \cdot v_i}{\sigma_i} \right)\trans \cdot
          \left( \frac{\mat{A} \cdot v_i}{\sigma_i} \right)
        = \frac{1}{\sigma_i^2} (v_i\trans \cdot \mat{A}\trans
          \cdot \mat{A} \cdot v_i)
        = \frac{1}{\sigma_i^2} (v_i\trans \cdot \sigma_i^2 \cdot v_i)
        = v_i\trans \cdot v_i
        = \lVert v_i \rVert_2^2
        = 1$.
    \item $\displaystyle i \neq j \Rightarrow u_i\trans \cdot u_j
        = \left( \frac{\mat{A} \cdot v_i}{\sigma_i} \right)\trans \cdot
          \left( \frac{\mat{A} \cdot v_j}{\sigma_j} \right)
        = \frac{v_i\trans \cdot \mat{A}\trans \cdot \mat{A} \cdot v_j}
          {\sigma_i \cdot \sigma_j}
        = \frac{v_i\trans \cdot \sigma_j^2 \cdot v_j}
          {\sigma_i \cdot \sigma_j}
        = \frac{\sigma_j}{\sigma_i} \cdot v_i\trans \cdot v_j
        = 0$.
    \end{itemize}

    Verifiquemos, también, que cumplan la ecuación (\ref{eq:svd:caso3}):
    \[ \mat{A}\trans \cdot u_i
        = \frac{\mat{A}\trans \cdot \mat{A} \cdot v_i}{\sigma_i}
        = \frac{\sigma_i^2 \cdot v_i}{\sigma_i}
        = \sigma_i \cdot v_i \]

\item Para $i = r+1, \dots, n$, ya vimos que, como $\mat{A}\trans \cdot
    \mat{A} \cdot v_i = 0$, también $\mat{A} \cdot v_i = 0$, por lo que
    se satisface la ecuación (\ref{eq:svd:caso2}).

    Aún no definimos $u_{r+1}, \dots, u_m$. Lo hacemos de modo que sean una
    base ortonormal de $\nul(\mat{A}\trans)$. Así, satisfacen la ecuación
    (\ref{eq:svd:caso4}): $\mat{A}\trans \cdot u_i = 0$.

    Solo falta ver que $\lbrace u_1, \dots, u_m \rbrace$ es una base
    ortonormal de $\reals^m$. Ahora bien:
    \begin{itemize}
    \item es una base, dado que $\lbrace u_1, \dots, u_r \rbrace$ es una base de
        $\im(\mat{A})$, $\lbrace u_{r+1},\dots,u_m \rbrace$ es una base de
        $\nul(\mat{A}\trans)$, y $\im(\mat{A}) \oplus \nul(\mat{A}\trans) =
        \reals^m$.
    \item ya vimos que tanto $\lbrace u_1, \dots, u_r \rbrace$ como
        $\im(\mat{A})$, $\lbrace u_{r+1},\dots,u_m \rbrace$ son conjuntos
        ortonormales. Basta ver, entonces, que tomando $i = 1,\dots,r$ y
        $j = r+1,\dots,m$, se cumple $u_i\trans \cdot u_j = 0$. En efecto,
        $\displaystyle u_i\trans \cdot u_j
            = \left( \frac{\mat{A} \cdot v_i}{\sigma_i} \right)\trans \cdot u_j
            = \frac{1}{\sigma_i} \cdot v_i\trans \cdot \mat{A}\trans \cdot u_j
            = \frac{1}{\sigma_i} \cdot v_i\trans \cdot 0
            = 0$.

    \end{itemize}

\end{enumerate}

% \subsubsection{Forma de Hessenberg}
% TODO: Averiguar bien qué es esto de Hessenberg. Me da la impresión de que
% es algo que no entra.
