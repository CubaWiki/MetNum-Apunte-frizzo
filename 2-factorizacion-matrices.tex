% -*- root: apunte-metodos.tex -*-

\section{Factorización de matrices}
\label{section:factorizacion-matrices}

En esta sección se presentarán tres maneras distintas de factorizar matrices.
La primera de ellas, la \textbf{factorización de Cholesky}, solo puede
aplicarse a matrices que cumplen con determinadas características; las otras
dos, la \textbf{factorización QR} y la \textbf{descomposición en valores
singulares} o \textbf{SVD}, son aplicables a matrices arbitrarias.

Una de las principales razones por las que estas factorizaciones resultan
útiles es porque proporcionan formas de resolver sistemas de ecuaciones
lineales que tienen propiedades deseables, como eficiencia, reutilizabilidad
y estabilidad numérica. No obstante, las factorizaciones QR y SVD también
tienen aplicaciones a la hora de resolver otros tipos de problemas numéricos,
como el problema de cuadrados mínimos lineales, que es abordado en la
sección \ref{section:cml}.

\subsection{Factorización de Cholesky}

Decimos que una matriz $\mat{A} \in \reals^{n \times n}$ es
\textbf{simétrica} si $\mat{A} = \mat{A}\trans$.
Si una matriz simétrica $\mat{A}$ es inversible y admite factorización LU,
entonces también admite un tipo de factorización que llamaremos LDL.
La misma consiste en una escritura
\[ \mat{A} = \mat{L} \cdot \mat{D} \cdot \mat{L}\trans \]
donde $\mat{L}$ es triangular inferior con unos en la diagonal y $\mat{D}$
es una matriz diagonal.

La demostración proviene de considerar la factorización LU de $\mat{A}$,
$\mat{A} = \mat{L} \cdot \mat{U}$. Como $\mat{L}$ tiene unos en la diagonal
debe ser inversible y, como $\mat{A}$ es inversible, $\mat{U}$ tiene que
serlo también; lo mismo vale para sus respectivas matrices traspuestas.
Es claro que 
\[ \mat{A} \ =\  \mat{L} \cdot \mat{U} \ =\ 
    \mat{L} \cdot \mat{U} \cdot (\mat{L}\trans)^{-1} \cdot \mat{L}\trans. \]
Definimos $\mat{D} = \mat{U} \cdot (\mat{L}\trans)^{-1}$, con lo que
$\mat{A} = \mat{L} \cdot \mat{D} \cdot \mat{L}\trans$. Resta ver que $\mat{D}$
es diagonal. Como $\mat{A}$ es simétrica, tenemos que
\[ \mat{L} \cdot \mat{U} = (\mat{L} \cdot \mat{U})\trans =
    \mat{U}\trans \cdot \mat{L}\trans. \]
Si multiplicamos a izquierda por $\mat{L}^{-1}$ y a derecha por
$(\mat{L}\trans)^{-1}$, obtenemos que
\[ \mat{U} \cdot (\mat{L}\trans)^{-1} = \mat{L}^{-1} \cdot \mat{U}\trans. \]
Ambos lados de la ecuación son iguales a $\mat{D}$. Del lado izquierdo de la
igualdad aparece un producto entre dos matrices triangulares superiores, que
es otra matriz triangular superior. Análogamente, del lado derecho se tiene
una matriz triangular inferior. Es decir, $\mat{D}$ es simultáneamente una
matriz triangular superior y triangular inferior, lo cual significa que es
una matriz diagonal.

Por otro lado, decimos que una matriz $\mat{A} \in \reals^{n \times n}$
es (simétrica y) \textbf{definida positiva} (s.d.p.) si es
simétrica\footnote{Algunos autores definen la noción de matriz definida
positiva independientemente de la matriz simétrica, mientras que otros
requieren la simetría como una condición para que una matriz sea definida
positiva; aquí se tomó el primer criterio, que es el adoptado por
\emph{Numerical Analysis} (9th Edition) de Richard L. Burden y J. Douglas
Faires.}
y para todo $\vec{x} \in \reals^{n}$ no nulo se cumple que
$\vec{x}\trans \cdot \mat{A} \cdot \vec{x} > 0$. Las matrices definidas positivas siempre son inversibles, y tienen elementos positivos en su diagonal.

Alternativamente, las matrices definidas positivas pueden caracterizarse como
aquellas matrices simétricas cuyos \textbf{menores
principales} son todos positivos. 
A partir de esta caracterización, se puede demostrar que las matrices
definidas positivas siempre admiten factorización LU, y por lo tanto,
al ser simétricas, poseen una factorización LDL.

Dada una matriz $\mat{A}$, llamamos \textbf{factorización de Cholesky} a su
escritura en la forma
\[ \mat{A} = \mat{L} \cdot \mat{L}\trans \]
donde $\mat{L}$ es una matriz triangular inferior con elementos positivos
en la diagonal. A continuación demostraremos que una matriz es
definida positiva si y solo si tiene factorización de Cholesky.

\begin{itemize}
\item[$(\Rightarrow)$] Consideremos una matriz definida positiva $\mat{A} \in
    \reals^{n \times n}$. Como enunciamos anteriormente, dicha matriz debe
    admitir una factorización LDL,
    $\mat{A} = \mat{L} \cdot \mat{D} \cdot \mat{L}\trans$.

    En primer lugar, se puede verificar que los elementos de la diagonal de
    $\mat{D}$ deben ser positivos.
    Basta considerar $i \in \{1, \dots, n\}$ cualquiera; como
    $\mat{L}\trans$ es inversible, se puede tomar $\vec{x} \in \reals^n$ tal
    que $\mat{L}\trans \cdot \vec{x} = \vec{e}_i$, donde $\vec{e}_i$ es un
    vector que tiene un uno en la posición $i$-ésima y ceros en
    las posiciones restantes. Luego,
    \[ \begin{aligned}
        0 &< \vec{x}\trans \cdot \mat{A} \cdot \vec{x} \\
          &= \vec{x}\trans \cdot \mat{L} \cdot \mat{D} \cdot \mat{L}\trans
            \cdot \vec{x} \\
          &= (\mat{L}\trans \cdot \vec{x})\trans \cdot \mat{D} \cdot
             (\mat{L}\trans \cdot \vec{x}) \\
          &= \vec{e}_i\trans \cdot \mat{D} \cdot \vec{e}_i \\
          &= d_{i,i}.
    \end{aligned} \]

    Esto permite definir
    \[ \mat{\sqrt{D}} = \begin{bmatrix}
        \sqrt{d_{1,1}} & 0              & \cdots & 0 \\
        0              & \sqrt{d_{2,2}} &        & 0 \\
        \vdots         &                & \ddots & \vdots \\
        0              & 0              & \cdots & \sqrt{d_{n,n}}
    \end{bmatrix} \]
    de modo que $\mat{D} = \mat{\sqrt{D}} \cdot \mat{\sqrt{D}} =
    \mat{\sqrt{D}} \cdot \mat{\sqrt{D}}\trans$. Así, reemplazando en la
    escritura LDL, obtenemos
    \[ \begin{aligned} \mat{A}
        &= \mat{L} \cdot \mat{D} \cdot \mat{L}\trans \\
        &= \mat{L} \cdot \mat{\sqrt{D}} \cdot \mat{\sqrt{D}}\trans
            \cdot \mat{L}\trans \\
        &= (\mat{L} \cdot \mat{\sqrt{D}}) \cdot
            (\mat{L} \cdot \mat{\sqrt{D}})\trans. \\
    \end{aligned} \]

    En esta última escritura, por las características de $\mat{L}$ y
    $\mat{\sqrt{D}}$, la matriz $\mat{L} \cdot \mat{\sqrt{D}}$ resulta
    triangular inferior con elementos positivos en la diagonal; por lo tanto,
    se trata de una factorización de Cholesky para $\mat{A}$.

\item[$(\Leftarrow)$] Si $\mat{A} = \mat{L} \cdot \mat{L}\trans$, entonces
    necesariamente $\mat{A}$ es simétrica. Basta ver que para todo $\vec{x}
    \neq \vec{0}$, $\vec{x}\trans \cdot \mat{A} \cdot \vec{x} > 0$.
    \[ \vec{x}\trans \cdot \mat{A} \cdot \vec{x}
        \ = \ \vec{x}\trans \cdot \mat{L} \cdot \mat{L}\trans \cdot \vec{x}
        \ = \ \left( \mat{L}\trans \cdot \vec{x} \right)\trans \cdot
            \left( \mat{L}\trans \cdot \vec{x} \right)
        \ = \ \Vert \mat{L}\trans \cdot \vec{x} \Vert_2^2, \]
    y al ser $\mat{L}\trans$ inversible,
    $\mat{L}\trans \cdot \vec{x} \neq \vec{0}$,
    por lo que la norma necesariamente es positiva.  
\end{itemize}

Una forma de computar la factorización de Cholesky de una matriz $\mat{A}$
surge de observar cómo se podría recuperar un elemento de $\mat{A}$ si ya
se contara con dicha factorización. Sea $\mat{A} = \mat{L} \cdot
\mat{L}\trans$ y consideremos, sin pérdida de generalidad, $i \geq j$; se puede notar que entonces
\[ a_{i,j} = \fil_i(\mat{L}) \cdot \col_j(\mat{L}\trans)
           = \sum_{k=1}^j l_{i,k} \cdot l_{j,k}. \]

La idea es recorrer la matriz $\mat{L}$ de forma ordenada, por columnas,
e ir utilizando la ecuación anterior para obtener, a partir de $\mat{A}$,
los valores que corresponden en cada posición.
\begin{itemize}
\item Para computar la esquina superior izquierda de $\mat{L}$, tenemos en
    cuenta que $a_{1,1} = (l_{1,1})^2$. Por lo tanto,
        \[ l_{1,1} = \sqrt{a_{1,1}}. \]
\item Para completar la primer columna, observamos que si $i > 1$, entonces
    $a_{i,1} = l_{i,1} \cdot l_{1,1}$. Luego,
    \[ l_{i,1} = \frac{a_{i,1}}{l_{1,1}}. \]
\item Consideremos ahora una columna $j > 1$. Como $\mat{L}$ es triangular
    inferior, el primer elemento a computar es el de la diagonal. Podemos
    observar que $a_{j,j} = \sum_{k=1}^j (l_{j,k})^2 =
    \sum_{k=1}^{j-1} (l_{j,k})^2 + (l_{j,j})^2$. Entonces,
    \[ l_{j,j} = \sqrt{a_{j,j} - \sum_{k=1}^{j-1} (l_{j,k})^2}. \]
\item Para calcular los restantes elementos, si $1 < j < i$, tenemos
    $a_{i,j} = \sum_{k=1}^j l_{i,k} \cdot l_{j,k} =
    \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} + l_{i,j} \cdot l_{j,j}$. Así,
    obtenemos que
    \[ l_{i,j} = \frac{1}{l_{j,j}} \cdot
        \left( a_{i,j} - \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} \right). \]
\end{itemize}

Si los pasos se siguen en orden, en cada uno de ellos solo se utilizan
elementos de $\mat{L}$ que ya fueron calculados previamente. Por lo tanto,
tenemos un algoritmo bien definido para obtener la factorización de Cholesky
de cualquier matriz definida positiva.
Además, el algoritmo determina $\mat{L}$ de forma
unívoca partiendo de una ecuación que debe ser satisfecha por toda
factorización de Cholesky de $\mat{A}$; esto demuestra que la factorización
de Cholesky es única.

A continuación, se presenta el algoritmo escrito en forma de pseudocódigo.

\begin{algorithm}[H]
\caption{Factorización de Cholesky}
\label{algo:cholesky}

\Input{$\mat{A} \in \reals^{n \times n}$ definida positiva.}
\Output{$\mat{L}$ triangular superior, con elementos positivos en la
    diagonal, tal que $\mat{A} = \mat{L} \cdot \mat{L}\trans$.}

$\displaystyle l_{1,1} \gets \sqrt{a_{1,1}}$\;
\For {$i=2,\dots,n$} {
    $\displaystyle l_{i,1} \gets \frac{a_{i,1}}{l_{1,1}}$\;
}
\For {$j=2,\dots,n$} {
    $\displaystyle l_{j,j} \gets \sqrt{a_{j,j} -
        \sum_{k=1}^{j-1} (l_{j,k})^2}$\;
    \For {$i=j+1,\dots,n$} {
        $\displaystyle l_{i,j} \gets \frac{1}{l_{j,j}} \cdot
        \left( a_{i,j} - \sum_{k=1}^{j-1} l_{i,k} \cdot l_{j,k} \right)$\;
    }
}
\end{algorithm}

Puede observarse que la complejidad del algoritmo es $\ord{n^3}$. Si bien
se trata de la misma complejidad asintótica que la de obtener una
factorización LU, las constantes son mejores; en la práctica, computar
una factorización de Cholesky es aproximadamente el doble de rápido que
obtener una factorización LU.

\newpage
\subsection{Factorización QR}

Dos vectores $\vec{x}, \vec{y} \in \reals^{n}$ se dicen \textbf{ortogonales}
($\vec{x} \bot \vec{y}$) si su producto interno
$\langle \vec{x},\vec{y} \rangle$ es $0$. Un conjunto de vectores
es \textbf{ortogonal} si sus elementos son ortogonales dos a dos. Un conjunto
de vectores es \textbf{ortonormal} si es ortogonal y la norma de todos sus
elementos es $1$. Los elementos de un conjunto ortonormal siempre son
linealmente independientes.

Decimos que una matriz $\mat{Q} \in \reals^{n \times n}$ es \textbf{ortogonal}
si sus columnas forman un conjunto ortonormal. Se puede demostrar, además, que
una matriz es ortogonal si y solo si es inversible y su transpuesta es igual a
su inversa, es decir, $\mat{Q}\trans = \mat{Q}^{-1}$. El producto de matrices
ortogonales es también ortogonal, es decir, si $\mat{Q_1}$ y $\mat{Q_2}$ son
ortogonales, $\mat{Q_1} \cdot \mat{Q_2}$ es ortogonal.

Las matrices ortogonales poseen la importante propiedad de estar asociadas a
las
transformaciones lineales que \emph{preservan norma 2}. En otras palabras, una
matriz $\mat{Q} \in \reals^{n \times n}$ es ortogonal si y solo si, para todo
$x \in \reals^n$, $\lVert \mat{Q} \cdot \vec{x} \rVert_2 =
\lVert \vec{x} \rVert_2$.
De esto se desprende también que el número de condición de una matriz
ortogonal (con respecto a la norma 2) es $\kappa(\mat{Q}) = 1$. Todo esto nos
indica que las matrices ortogonales son muy estables numéricamente, lo cual
las vuelve muy atractivas a la hora de resolver problemas en forma
computacional.

Dada una matriz $\mat{A} \in \reals^{m \times n}$, una
\textbf{factorización QR} de $\mat{A}$ es una escritura
\[ \mat{A} = \mat{Q} \cdot \mat{R} \]
donde $\mat{Q} \in \reals^{m \times m}$ es ortogonal y $\mat{R} \in \reals^{m
\times n}$ es triangular superior. Toda matriz admite una factorización QR,
incluso aquellas que no son cuadradas, como se verá más adelante al analizar
los algoritmos que permiten obtenerla.

Con respecto a la unicidad de la factorización QR, puede asegurarse para
matrices inversibles si se agrega la restricción de que los elementos
en la diagonal de $\mat{R}$ sean positivos.

Una de las aplicaciones clave de esta factorización es la resolución
de sistemas de ecuaciones lineales. Si se tiene el sistema
$\mat{A} \cdot \vec{x} = \vec{b}$,
utilizando la factorización QR, se lo puede reescribir como
\[\mat{Q} \cdot \mat{R} \cdot \vec{x} = \vec{b}\]
Al ser $\mat{Q}$ ortogonal, se puede multiplicar
a ambos lados por $\mat{Q}\trans$ y se tiene
\[\mat{Q} \cdot \trans \mat{Q} \cdot \mat{R} \cdot \vec{x}
    = \mat{Q}\trans \vec{b}\]
o lo que es lo mismo,
\[\mat{R} \cdot \vec{x} = \mat{Q}\trans \cdot \vec{b}\]
Como $\mat{R}$ es triangular superior, este sistema se puede resolver de forma
eficiente con sustitución hacia atrás, con la importante ventaja de que
$\mat{Q}\trans$ es sumamente estable, evitando la introducción de errores
numéricos al calcular el producto con $\vec{b}$.

\subsubsection{Método de Householder (Reflexiones)}
Las dos formas que estudiaremos para computar la factorización QR de una
matriz se basan en el mismo principio: ir aplicando sucesivas transformaciones
a la matriz, todas ellas definidas por matrices ortogonales, hasta llevarla
a una forma triangular superior. La diferencia está en el tipo de estas
transformaciones. En el caso del \textbf{método de Householder}, se trata
de \textbf{matrices de reflexión}.

Una matriz de reflexión en $\reals^{n \times n}$ envía a cualquier vector
a su opuesto por un determinado hiperplano de $\reals^{n \times n}$, es
decir, un subespacio de dimensión $n-1$. Intuitivamente, se trata de una
transformación que no altera la norma 2 de los vectores y, por lo tanto,
la matriz asociada a ella debe ser ortogonal.

Cualquier hiperplano se puede determinar unívocamente mediante
un vector no nulo $\vec{u} \in \reals^{n}$ ortogonal a él, para el cual la
matriz de reflexión $\mat{W}$ cumplirá $\mat{W} \cdot \vec{u} = - \vec{u}$.
Por otro lado, para cualquier $\vec{v}$ que sea ortogonal a $\vec{u}$ (o, lo
que es lo mismo, que sea parte del hiperplano), debe suceder que
$\mat{W} \cdot \vec{v} = \vec{v}$.

Partiendo de las dos condiciones anteriores puede demostrarse que, si
se toma $\vec{u}$ unitario (es decir, $\Vert \vec{u} \Vert_2 = 1$),
la matriz de reflexión correspondiente al hiperplano
ortogonal a $\vec{u}$ es
\[ \mat{W} = \mat{I} - 2 \cdot \vec{u} \cdot \vec{u}\trans. \]
Además, la matriz $\mat{W}$ resulta ortogonal.

Un resultado que nos será útil es que, si se tienen dos vectores $\vec{v}$ y
$\vec{w} \in \reals^n$ de igual norma 2, siempre es posible encontrar un
hiperplano tal que la reflexión de $\vec{v}$ en el mismo sea $\vec{w}$.
Dicho hiperplano será perpendicular a la recta que pasa por los extremos de
$\vec{v}$ y $\vec{w}$, por lo que se lo puede describir mediante el versor
\[ \vec{u} = \frac{\vec{v} - \vec{w}}{\Vert \vec{v} - \vec{w} \Vert_2}. \]

Partiendo de esta idea, buscaremos tomar una matriz
$\mat{A} \in \reals^{m \times n}$ y usar reflexiones sucesivas para
triangularla, enviando cada una de sus columnas a
un vector de igual norma, pero que tenga ceros en los lugares donde sea
necesario.

Para describir el proceso, fijamos $\mat{A}^{(0)} = \mat{A}$; luego, para
cada $k \in \{1,\dots,n-1\}$, consideremos la matriz $\mat{A}^{(k-1)}$
obtenida en el paso anterior.
El objetivo del paso $k$-ésimo del algoritmo será poner ceros debajo de
la diagonal en la $k$-ésima columna de la matriz. Podemos notar que
\[
\mat{A}^{(k-1)} = \left[ \hspace{.1cm} \begin{array}{ccc|ccc}
    \ast & \cdots  & \ast   &         &            &         \\
         & \ddots  & \vdots &         & \textbf{*} &         \\
         &         & \ast   &         &            &         \\ \hline
         &         &        & w_{k,k} & \cdots     & w_{k,n} \\
         & \mat{0} &        & \vdots  & \ddots     & \vdots  \\
         &         &        & w_{m,k} & \cdots     & w_{m,n} \\
\end{array} \hspace{.1cm} \right]
\]

Teniendo en cuenta solamente la submatriz que todavía no fue triangulada,
podemos lograr nuestro objetivo buscando una reflexión $\mat{\tilde W}_k
\in \reals^{(m-k+1) \times (m-k+1)} $ tal que
\[
\mat{\tilde W}_k \cdot
    \begin{bmatrix} w_{k,k} \\ w_{k+1,k} \\ \vdots \\ w_{m,k} \end{bmatrix}
=  \begin{bmatrix} \Vert (w_{k,k}, \dots, w_{m,k}) \Vert \\ 0 \\ \vdots \\ 0 \end{bmatrix}
\]
Llamando $\vec{v}$ y $\vec{w}$ a los dos vectores que aparecen en la ecuación
anterior, respectivamente, podemos definir el vector unitario
$\vec{\tilde u}_k = \frac{\vec{v} - \vec{w}}{\Vert \vec{v} - \vec{w} \Vert_2}$
y así la matriz de reflexión asociada $\mat{\tilde W}_k = \mat{I} - 2 \cdot
\vec{\tilde u} \cdot \vec{\tilde u}\trans$ cumple con lo que necesitamos.
Si ahora completamos con ceros las primeras $k-1$ posiciones de $\vec{\tilde u}_k$, definiendo $\vec{u}_k = ( \begin{array}{c|c} \vec{0} & \vec{\tilde u}_k \end{array} )$, la matriz de reflexión asociada $\mat{W}_k = \mat{I} - 2 \cdot
\vec{u} \cdot \vec{u}\trans \in \reals^{m \times m}$ tiene la forma
\[
\mat{W}_k = \left[ \hspace{.1cm} \begin{array}{c|c}
    \matbox{.6cm}{1cm}{\mat{I}_{k-1}}
        & \matbox{.6cm}{1cm}{\mat{0}} \\ \hline
    \matbox{.6cm}{1cm}{\mat{0}}
        & \matbox{.6cm}{1cm}{\mat{\tilde W}_k}
\end{array} \hspace{.1cm} \right],
\]
por lo que $\mat{A}^{(k)} = \mat{W}_k \cdot \mat{A}^{(k-1)}$ mantendrá sin
cambios sus primeras $k-1$ columnas, pero también tendrá ceros debajo de la
diagonal en la columna $k$-ésima.

Podemos concluir, entonces, que la matriz $\mat{A}^{(n-1)}$ será triangular
superior. Resumiendo lo hecho en todos los pasos, y llamando
$\mat{R} = \mat{A}^{(n-1)}$, tenemos que:
\[ \begin{aligned}
    \mat{R} = \mat{A}^{(n-1)}
    &= \mat{W}_{n-1} \cdot \ldots \cdot \mat{W}_{1}
        \cdot \mat{A} \\
    &= \mat{W} \cdot \mat{A}
\end{aligned} \]
donde $\mat{W}$, el producto de todas las $\mat{W}_k$, es también ortogonal
por ser el producto de matrices ortogonales.
Finalmente, si definimos $\mat{Q} = \mat{W}^{-1} = \mat{W}\trans$, que es
también ortogonal, resulta que
\[ \mat{A} = \mat{Q} \cdot \mat{R}, \]
es decir, hemos obtenido una factorización QR para $\mat{A}$.

La complejidad del método de Householder es cúbica en el tamaño de la
matriz ($\ord{n^3}$); si se realiza un análisis más fino, se puede verificar
que la cantidad de operaciones de punto flotante necesarias es de alrededor
de $\frac{2}{3} \cdot n^3$.

\subsubsection{Método de Givens (Rotaciones)}
El \textbf{método de Givens}, por su parte, realiza las transformaciones
a través de un tipo de matrices ortogonales conocidas como
\textbf{matrices de rotación}.

Las matrices de rotación reciben este nombre porque la transformación
lineal inducida por ellas hace rotar a cualquier vector, en torno al origen,
un ángulo fijo $\theta$. Es evidente que las transformaciones de este tipo
preservan la longitud de los vectores, por lo que las matrices
correspondientes son ortogonales.

Por ejemplo, si se consideran vectores en $\reals^2$ y un ángulo $\theta$,
se puede definir la matriz de rotación $\mat{W}$ correspondiente a dicho
ángulo\footnote{Para construir las matrices de rotación, interpretaremos
que los ángulos expresan giros en el sentido de las agujas del reloj.}
a partir de los valores que deberá tomar la transformación en una base de
$\reals^2$.
Se quiere que
\[
\mat{W} \cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} =
    \begin{bmatrix}\cos(\theta) \\ -\sin(\theta) \end{bmatrix}
\qquad \text{y} \qquad
\mat{W} \cdot \begin{bmatrix} 0 \\ 1 \end{bmatrix} =
    \begin{bmatrix}\sin(\theta) \\ \cos(\theta) \end{bmatrix},
\]
por lo que
\[
\mat{W} = \begin{bmatrix}
    \cos(\theta)   & \sin(\theta) \\
    - \sin(\theta) & \cos(\theta)
\end{bmatrix}.
\]
Es sencillo verificar que, para cualquier valor de $\theta$, esta matriz es
ortogonal.

Nuestro objetivo es usar sucesivas rotaciones para ir eliminando componentes
de las columnas de una matriz, hasta lograr triangularla. En el caso de una
matriz
$\mat{A} \in \reals^{2 \times 2}$,
\[
\mat{A} = \begin{bmatrix}
    a_{1} & b_{1} \\
    a_{2} & b_{2}
\end{bmatrix},
\]
queremos rotar la primera columna de modo tal que su segunda coordenada
pase a tener un $0$; en otras palabras, la columna debe pasar a ser un vector
paralelo al eje de abscisas. Adoptaremos la convención de que apuntará
en el sentido positivo. Luego, como las rotaciones preservan la norma 2,
buscamos una matriz de rotación que cumpla
\[
\mat{W} \cdot \begin{bmatrix} a_{1} \\ a_{2} \end{bmatrix} =
    \begin{bmatrix} \Vert (a_{1}, a_{2}) \Vert_2 \\ 0 \end{bmatrix}.
\]
Se puede ver que el ángulo $\theta$ de la rotación deberá ser tal que
\[ \cos(\theta) = \frac{a_1}{\Vert (a_{1}, a_{2}) \Vert_2}
    \qquad \text{y} \qquad
    \sin(\theta) = \frac{a_2}{\Vert (a_{1}, a_{2}) \Vert_2}. \]
Por lo tanto,
\[
\mat{W} = \begin{bmatrix}
    \vvecbox{.7cm}{\dfrac{a_1}{\Vert (a_{1}, a_{2}) \Vert_2}}
        & \vvecbox{.7cm}{\dfrac{a_2}{\Vert (a_{1}, a_{2}) \Vert_2}} \\
    - \vvecbox{.7cm}{\dfrac{a_2}{\Vert (a_{1}, a_{2}) \Vert_2}}
        & \vvecbox{.7cm}{\dfrac{a_1}{\Vert (a_{1}, a_{2}) \Vert_2}}
\end{bmatrix}.
\]
Aplicando esta rotación sobre $\mat{A}$, se obtiene una matriz $\mat{R}$
triangular inferior. Como $\mat{W} \cdot \mat{A} = \mat{R}$, entonces
$\mat{A} = \mat{W}^{-1} \cdot \mat{R} = \mat{W}\trans \cdot \mat{R}$. Dado
que $\mat{W}\trans$ es ortogonal, llamando $\mat{Q} = \mat{W}\trans$,
tenemos una factorización QR para $\mat{A}$.

Esto se puede generalizar a matrices $\mat{A} \in \reals^{m \times n}$. La
idea es iterar sobre las columnas de la matriz, e ir rotándolas hasta que
contengan ceros en las posiciones deseadas. Para cada columna, se realizarán
varias rotaciones sucesivas, tantas como ceros sea necesario colocar en ella.

Más en detalle, si se está trabajando con la columna $j$-ésima, para cada
$i>j$ se definirá $\mat{W}_{i,j} \in \reals^{m \times m}$
de la siguiente forma:
\[ \begin{aligned}
    w_{i,i} &= \dfrac{a_i}{\Vert (a_{i}, a_{j}) \Vert_2}, \qquad&
    w_{i,j} &= \dfrac{a_j}{\Vert (a_{i}, a_{j}) \Vert_2}, \\
    w_{j,i} &= - \dfrac{a_j}{\Vert (a_{i}, a_{j}) \Vert_2}, \qquad&
    w_{j,j} &= \dfrac{a_i}{\Vert (a_{i}, a_{j}) \Vert_2},
\end{aligned} \]
y el resto de las posiciones al igual que la matriz identidad. Los valores de
las posiciones de $\mat{A}$ se van modificando a lo largo del proceso, por lo
que siempre se consideran los obtenidos en la iteración anterior del
algoritmo. Es sencillo verificar que cada $\mat{W}_{i,j}$ es una
matriz ortogonal.

De esta forma, construyendo iterativamente las matrices de rotación y
multiplicando $\mat{A}$ a izquierda, se llega a una forma triangular superior
\[ \begin{aligned}
    \mat{R} &= (\mat{W}_{n,n-1})
        \cdot (\mat{W}_{n,n-2} \cdot \mat{W}_{n-1,n-2})
        \cdot \ldots
        \cdot (\mat{W}_{n,1} \cdot \ldots \cdot \mat{W}_{2,1}) \cdot \mat{A}. \\
    &= \mat{W} \cdot \mat{A}
\end{aligned} \]
donde $\mat{W}$ es el producto de todas las $\mat{W}_{i,j}$, y es por lo tanto
una matriz ortogonal. Tomando $\mat{Q} = \mat{W}\trans$, se obtiene
\[ \mat{A} = \mat{Q} \cdot \mat{R}, \]
una factorización QR para $\mat{A}$.

En comparación con el método de Householder, el método de Givens, si bien
tiene la misma complejidad asintótica ($\ord{n^3}$), debe realizar
aproximadamente el doble de operaciones de punto flotante: alrededor de
$\frac{4}{3} \cdot n^3$. No obstante, presenta una gran ventaja si se está
trabajando con matrices ralas (donde una gran cantidad de las posiciones está
ocupada por ceros), ya que cada paso del algoritmo pone un cero en una
posición de la matriz, por lo que se puede realizar una optimización colocando
ceros solo en las posiciones donde es necesario. En contraste, al usar el
método de Householder, cada iteración pone ceros en una columna completa.

\newpage
\subsection{Descomposición en valores singulares}

A partir de esta sección, se hace necesario presentar dos nociones importantes
que continuarán apareciendo más adelante.
Dada una matriz $\mat{A} \in \reals^{n \times n}$, decimos que $\lambda \in
\reals$ es un \textbf{autovalor} de $\mat{A}$ si existe algún vector no nulo
$\vec{v} \in \reals^n$ tal que $\mat{A} \cdot \vec{v} =
\lambda \cdot \vec{v}$.
En ese caso, a $\vec{v}$ se lo conoce como un \textbf{autovector} de $\mat{A}$
asociado al autovalor $\lambda$. Cabe destacar que, si $\lambda$ es un
autovalor de una matriz, entonces existen infinitos autovectores asociados al
mismo; los autovectores asociados a un autovalor $\lambda$ dado conforman un
subespacio vectorial de $\reals^{n \times n}$.

Sea $\mat{A} \in \reals^{m \times n}$ una matriz cualquiera, con $\rg(A) = r$.
Una \textbf{descomposición en valores singulares} de $\mat{A}$
es una escritura
\[ \mat{A} = \mat{U} \cdot \mat{\Sigma} \cdot \mat{V}\trans, \]
donde
\begin{itemize} \vspace{.6em}
\item $\mat{U} = \begin{bmatrix}
                  &           &        &           \\
        \vec{u}_1 & \vec{u}_2 & \dots  & \vec{u}_m \\
                  &           &        &           \\
    \end{bmatrix} \in \reals^{m \times m}$ y
    $\mat{V} = \begin{bmatrix}
                  &           &        &           \\
        \vec{v}_1 & \vec{v}_2 & \dots  & \vec{v}_n \\
                  &           &        &           \\
    \end{bmatrix} \in \reals^{n \times n}$ son ortogonales, y \vspace{.6em}
\item $\mat{\Sigma} = \begin{bmatrix}
\sigma_1 & 0        & \cdots   &    &          &          &          \\
0        & \sigma_2 &          &          &          &          &          \\
\vdots   &          & \ddots   &          &          &          &          \\
         &          &          & \sigma_r &          &          &          \\
         &          &          &          & 0        &          &          \\
         &          &          &          &          & \ddots   &          \\
         &          &          &          &          &          & 0
\end{bmatrix} \in \reals^{m \times n}$ es diagonal,
    con $\sigma_i > 0$ para todo $i \in \{ 1, \dots, r \}$.
\end{itemize} \vspace{.6em}

Los valores $\sigma_i$ se denominan los \textbf{valores singulares} de
\mat{A}. Por su parte, $\lbrace \vec{u}_1, \vec{u}_2, \dots, \vec{u}_m
\rbrace$ y $\lbrace \vec{v}_1, \vec{v}_2, \dots, \vec{v}_n \rbrace$ son bases
ortonormales de $\reals^m$ y $\reals^n$, respectivamente.

Como se verá en la demostración de existencia que se presenta a continuación,
se cumple que:
\begin{itemize}
\item $\sigma_1^2, \dots, \sigma_r^2$ son los autovalores no nulos tanto de
    $\mat{A} \cdot \mat{A}\trans$ como de $\mat{A}\trans \cdot \mat{A}$.
\item Para cada $i \in \{1, \dots, r\}$, $\vec{u}_i$ es un autovector unitario
    de $\mat{A} \cdot \mat{A}\trans$ asociado al autovalor $\sigma_i^2$,
    mientras que $\vec{v}_i$ es un autovector unitario de $\mat{A}\trans \cdot
    \mat{A}$ asociado al mismo autovalor.
\item Los $\vec{u}_i$ y $\vec{v}_i$ con $i > r$ forman parte del núcleo de
    $\mat{A} \cdot \mat{A}\trans$ y de $\mat{A}\trans \cdot \mat{A}$,
    respectivamente.
\end{itemize}

La descomposición en valores singulares tiene un interés muy particular en
diversas aplicaciones específicas, ya que logra condensar en pocos valores
(los valores singulares) información muy relevante acerca de la matriz con la que se está trabajando. Un campo en el que suele tener aplicación es el de la
estadística. Por otro lado, es una factorización útil a la hora de resolver
el problema de cuadrados mínimos lineales, que será abordado en la sección
\ref{section:cml}.

\subsubsection{Demostración de la existencia}

Es posible demostrar que toda matriz admite una descomposición en valores
singulares. Para demostrarlo, comenzamos por observar que la igualdad
enunciada,
\[ \mat{A} = \mat{U} \cdot \mat{\Sigma} \cdot \mat{V}\trans, \]
puede expresarse de forma equivalente mediante las siguientes ecuaciones,
usando la ortogonalidad de $\mat{U}$ y $\mat{V}$:
\begin{multicols}{2}\noindent
\[ \begin{aligned} \mat{A} \cdot \mat{V} &= \mat{U} \cdot \mat{\Sigma} \\
    & \Updownarrow \\
    \mat{A} \cdot \begin{bmatrix}
                  &        &           \\
        \vec{v}_1 & \dots  & \vec{v}_n \\
                  &        &           \\
    \end{bmatrix} &= \begin{bmatrix}
                  &        &           \\
        \vec{u}_1 & \dots  & \vec{u}_m \\
                  &        &           \\
    \end{bmatrix} \cdot \mat{\Sigma} \\
    & \Updownarrow
\end{aligned}
\]
\begin{numcases}{}
    \mat{A} \cdot \vec{v}_i = \sigma_i \cdot \vec{u}_i
        & para $i = 1,\dots,r$
        \label{eq:svd:caso1} \\
    \mat{A} \cdot \vec{v}_i = \vec{0}
        & para $i = r+1,\dots,n$
        \label{eq:svd:caso2}
\end{numcases}

\columnbreak \noindent
\[\begin{aligned} \mat{A}\trans \cdot \mat{U} &= \mat{V} \cdot \mat{\Sigma} \\
    & \Updownarrow \\
    \mat{A}\trans \cdot \begin{bmatrix}
                  &        &           \\
        \vec{u}_1 & \dots  & \vec{u}_m \\
                  &        &           \\
    \end{bmatrix} &= \begin{bmatrix}
                  &        &           \\
        \vec{v}_1 & \dots  & \vec{v}_n \\
                  &        &           \\
    \end{bmatrix} \cdot \mat{\Sigma} \\
    & \Updownarrow
\end{aligned}
\]
\begin{numcases}{}
    \mat{A}\trans \cdot \vec{u}_i = \sigma_i \cdot \vec{v}_i
        & para $i = 1,\dots,r$
        \label{eq:svd:caso3} \\
    \mat{A}\trans \cdot \vec{u}_i = \vec{0}
        & para $i = r+1,\dots,m$
        \label{eq:svd:caso4}
\end{numcases}

\end{multicols}

En otras palabras, queremos probar la existencia de
\begin{itemize}
\item $\vec{v}_1, \dots, \vec{v}_n \in \reals^n$ ortonormales,
\item $\vec{u}_1, \dots, \vec{u}_m \in \reals^m$ ortonormales, y
\item $\sigma_1, \dots, \sigma_r > 0$
\end{itemize}
que cumplan las ecuaciones (\ref{eq:svd:caso1}), (\ref{eq:svd:caso2}),
(\ref{eq:svd:caso3}) y (\ref{eq:svd:caso4}).

En primer lugar, vemos que:
\begin{enumerate}[label=(\roman*)]
\item Para $i = 1, \dots, r$,
\[ \begin{aligned}
    \mat{A}\trans \cdot \mat{A} \cdot \vec{v}_i
        &= \sigma_i \cdot \mat{A}\trans \cdot \vec{u}_i \\
        &= \sigma_i^2 \cdot \vec{v}_i
\end{aligned} \]
es decir, los $\vec{v}_1,\dots,\vec{v}_r$ deberán ser autovectores de
$\mat{A}\trans \cdot \mat{A}$, con autovalor $\sigma_i^2$.
\item Para $i = r+1, \dots, n$,
\[ \mat{A}\trans \cdot \mat{A} \cdot \vec{v}_i = \mat{A}\trans \cdot 0 = 0 \]
es decir, los $\vec{v}_{r+1},\dots,\vec{v}_n$ deberán ser autovectores de
$\mat{A}\trans \cdot \mat{A}$, con autovalor $0$.
\end{enumerate}

Como $\mat{A}\trans \cdot \mat{A}$ es simétrica, existe una base ortonormal de
$\reals^n$ compuesta por autovectores de $\mat{A}\trans \cdot \mat{A}$,
propiedad que utilizaremos sin demostración. Esto implica que todos los
autovalores de $\mat{A}$ son reales.

Además, como $\mat{A}$ es semidefinida positiva, todos sus autovalores deben
ser no negativos: si $\lambda$ es un autovector de $\mat{A}$, entonces existe
un vector no nulo $\vec{v}$ tal que $\mat{A} \cdot \vec{v} = \lambda \cdot
\vec{v}$. Entonces $\vec{v}\trans \cdot \mat{A} \cdot \vec{v} = \lambda \cdot
\vec{v}\trans \cdot \vec{v}$. Por ser $\mat{A}$ semidefinida positiva,
$\vec{v}\trans \cdot \mat{A} \cdot \vec{v} \geq 0$, mientras que
$\vec{v}\trans \cdot \vec{v} = \lVert \vec{v} \rVert_2^2 > 0$, de forma que
necesariamente $\lambda \geq 0$.

Por otra parte, tenemos que para todo $\vec{x} \in \reals^n$
\[ \mat{A} \cdot \vec{x} = \vec{0} \quad \Leftrightarrow \quad
    \mat{A}\trans \cdot \mat{A} \cdot \vec{x} = \vec{0} \]
dado que, trivialmente, si $\mat{A} \cdot \vec{x} = \vec{0}$ entonces
$\mat{A}\trans \cdot \mat{A} \cdot \vec{x} = \vec{0}$, mientras que si
$\mat{A}\trans \cdot \mat{A} \cdot \vec{x} = \vec{0}$ entonces
$\vec{x}\trans \cdot \mat{A}\trans \cdot \mat{A} \cdot \vec{x}
    = \lVert \mat{A} \cdot \vec{x} \rVert_2^2 = 0$, lo cual solo sucede si
$\mat{A} \cdot \vec{x} = \vec{0}$.
Así, $\rg(\mat{A}\trans \cdot \mat{A}) = \rg(\mat{A}) = r$, lo cual significa
que $\nul(\mat{A}\trans \cdot \mat{A})$ tiene dimensión $n-r$; en otras
palabras $\mat{A}\trans \cdot \mat{A}$ tiene exactamente $n - r$ autovectores
linealmente independientes asociados al autovalor $0$.

Definimos entonces $\lbrace \vec{v}_1, \dots, \vec{v}_n \rbrace$ como una base
ortonormal de autovectores de $\mat{A}\trans \cdot \mat{A}$, ordenados de modo
que los últimos $n-r$ son los asociados al autovalor $0$.
Veamos que estos $\vec{v}_i$ cumplen con las condiciones necesarias.

\begin{enumerate}[label=(\roman*)]
\item Para $i = 1, \dots, r$, sea $\lambda_i \in \reals$
    el autovalor de $\mat{A}\trans \cdot \mat{A}$
    asociado al autovector $\vec{v}_i$.
    Sabemos que $\lambda_i > 0$.
    Definimos entonces:
    \begin{itemize}
    \item $\sigma_i = \sqrt{\lambda_i} > 0$.
    \item $\displaystyle \vec{u}_i = \frac{\mat{A} \cdot \vec{v}_i}{\sigma_i}$.
        Trivialmente, puede verse que se satisface la ecuación
        (\ref{eq:svd:caso1}).
    \end{itemize}

    Veamos que $\vec{u}_1, \dots, \vec{u}_r$ son ortonormales:
    \begin{itemize}
    \item $\displaystyle \lVert \vec{u}_i \rVert_2^2
        = \left( \frac{\mat{A} \cdot \vec{v}_i}{\sigma_i} \right)\trans \cdot
          \left( \frac{\mat{A} \cdot \vec{v}_i}{\sigma_i} \right)
        = \frac{1}{\sigma_i^2} (\vec{v}_i\trans \cdot \mat{A}\trans
          \cdot \mat{A} \cdot \vec{v}_i)
        = \frac{1}{\sigma_i^2} (\vec{v}_i\trans \cdot \sigma_i^2
          \cdot \vec{v}_i)
        = \vec{v}_i\trans \cdot \vec{v}_i
        = \lVert \vec{v}_i \rVert_2^2
        = 1$.
    \item Si $i \neq j$, entonces $\begin{aligned}[t]
        \displaystyle \vec{u}_i\trans \cdot \vec{u}_j
        = \left( \frac{\mat{A} \cdot \vec{v}_i}{\sigma_i} \right)\trans \cdot
          \left( \frac{\mat{A} \cdot \vec{v}_j}{\sigma_j} \right)
        &= \frac{\vec{v}_i\trans \cdot \mat{A}\trans \cdot \mat{A}
          \cdot \vec{v}_j}{\sigma_i \cdot \sigma_j} \\
        &= \frac{\vec{v}_i\trans \cdot \sigma_j^2 \cdot \vec{v}_j}
          {\sigma_i \cdot \sigma_j}
        = \frac{\sigma_j}{\sigma_i} \cdot \vec{v}_i\trans \cdot \vec{v}_j
        = 0. \end{aligned}$
    \end{itemize}

    Resta verificar que se cumple la ecuación (\ref{eq:svd:caso3}):
    \[ \mat{A}\trans \cdot \vec{u}_i
        = \frac{\mat{A}\trans \cdot \mat{A} \cdot \vec{v}_i}{\sigma_i}
        = \frac{\sigma_i^2 \cdot \vec{v}_i}{\sigma_i}
        = \sigma_i \cdot \vec{v}_i \]

\item Para $i = r+1, \dots, n$, ya vimos que, como $\mat{A}\trans \cdot
    \mat{A} \cdot \vec{v}_i = \vec{0}$, también $\mat{A} \cdot \vec{v}_i
    = \vec{0}$, por lo que se satisface la ecuación (\ref{eq:svd:caso2}).

    Aún no definimos $\vec{u}_{r+1}, \dots, \vec{u}_m$.
    Lo hacemos de modo que sean una base ortonormal de $\nul(\mat{A}\trans)$.
    Así, se satisface la ecuación
    (\ref{eq:svd:caso4}): $\mat{A}\trans \cdot \vec{u}_i = \vec{0}$.

    Solo falta ver que $\lbrace \vec{u}_1, \dots, \vec{u}_m \rbrace$ es una
    base ortonormal de $\reals^m$. Ahora bien:
    \begin{itemize}
    \item es una base, dado que $\lbrace \vec{u}_1, \dots, \vec{u}_r \rbrace$
        es una base de $\im(\mat{A})$,
        $\lbrace \vec{u}_{r+1},\dots,\vec{u}_m \rbrace$ es una base de
        $\nul(\mat{A}\trans)$, y $\im(\mat{A}) \oplus \nul(\mat{A}\trans) =
        \reals^m$.
    \item ya vimos que tanto $\lbrace \vec{u}_1, \dots, \vec{u}_r \rbrace$
        como $\im(\mat{A})$, $\lbrace \vec{u}_{r+1},\dots,\vec{u}_m \rbrace$
        son conjuntos ortonormales. Basta ver, entonces, que tomando
        $i \in \{1,\dots,r\}$ y $j \in \{r+1,\dots,m\}$,
        se cumple que $\vec{u}_i \bot \vec{u}_j$, es decir, que
        $\vec{u}_i\trans \cdot \vec{u}_j = 0$. En efecto,
        \[ \vec{u}_i\trans \cdot \vec{u}_j
            = \left( \frac{\mat{A} \cdot \vec{v}_i}{\sigma_i} \right)\trans
                \cdot \vec{u}_j
            = \frac{1}{\sigma_i} \cdot \vec{v}_i\trans \cdot \mat{A}\trans
                \cdot \vec{u}_j
            = \frac{1}{\sigma_i} \cdot \vec{v}_i\trans \cdot \vec{0}
            = 0. \]
    \end{itemize}
\end{enumerate}

% TODO: Aplicaciones y otras yerbas
